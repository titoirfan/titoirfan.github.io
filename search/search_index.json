{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#irfan-tito-kurniawan","title":"Irfan Tito Kurniawan","text":"<p>Hi there!</p> <p>I'm Tito (\u30c6\u30a3\u30c8), currently pursuing my PhD at Tohoku University's Neuro-Robotics Lab. My research interest lies in the field of robot learning, aiming to enable legged robots to intelligently navigate the real world. Aside from robotics and deep learning, I also have some background in computer vision.</p> <p>On my free time, I like to draw illustrations, study Japanese, and play video games. I also have a blog, about, and projects page on this website, please check it if you're interested.</p>"},{"location":"#contact","title":"Contact","text":"<p>Feel free to reach me via email -  irfantitok@gmail.com</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#personal-information","title":"Personal Information","text":"<ul> <li>Name: Irfan Tito Kurniawan (\u30a4\u30eb\u30d5\u30a1\u30f3\u30fb\u30c6\u30a3\u30c8\u30fb\u30af\u30eb\u30cb\u30a2\u30ef\u30f3)</li> <li>Email: irfantitok@gmail.com</li> <li>Google Scholar: Link</li> </ul> <p>I am a PhD student at Tohoku University, my research interests include robotics and computer vision.</p>"},{"location":"about/#education","title":"Education","text":"<ul> <li> <p>PhD in Engineering</p> <ul> <li>Department of Robotics, Graduate School of Engineering, Tohoku University, Sendai, Japan</li> <li>Duration: Oct 2024 - Sep 2027 (Expected)</li> <li>Major: Robotics</li> <li>Lab: Neuro-Robotics Lab</li> </ul> </li> <li> <p>MSc in Electrical Engineering</p> <ul> <li>School of Electrical Engineering and Informatics, Bandung Institute of Technology, Bandung, Indonesia</li> <li>Duration: Aug 2021 - Apr 2023</li> <li>Major: Electrical Engineering (Control and Intelligent Systems)</li> <li>Lab: Advanced Robotics Lab</li> <li>GPA: 3.79/4.00 (Cum Laude)</li> </ul> </li> <li> <p>BSc in Biomedical Engineering</p> <ul> <li>School of Electrical Engineering and Informatics, Bandung Institute of Technology, Bandung, Indonesia</li> <li>Duration: Aug 2017 - Jul 2021</li> <li>Major: Biomedical Engineering</li> <li>GPA: 3.78/4.00 (Cum Laude)</li> </ul> </li> </ul>"},{"location":"about/#selected-publications","title":"Selected Publications","text":"<ul> <li> <p>I. T. Kurniawan and B. R. Trilaksono, \u201dClusterFusion: Leveraging Radar Spatial Features for Radar-Camera 3D Object Detection in Autonomous Vehicles,\u201d in IEEE Access, vol. 11, pp. 121511-121528, 2023, doi: 10.1109/ACCESS.2023.3328953.</p> </li> <li> <p>I. T. Kurniawan and B. R. Trilaksono, \u201dImproving Radar-Camera Fusion-based 3D Object Detection for Autonomous Vehicles,\u201d 2022 12th International Conference on System Engineering and Technology (ICSET), Bandung, Indonesia, 2022, pp. 42-47, doi: 10.1109/ICSET57543.2022.10011030.</p> </li> <li> <p>I. T. Kurniawan and W. Adiprawita, \u201dA Method of Ultraviolet-C Surface Irradiation Simulation and Evaluation,\u201d 2021 International Symposium on Electronics and Smart Devices (ISESD), Bandung, Indonesia, 2021, pp. 1-5, doi: 10.1109/ISESD53023.2021.9501868.</p> </li> <li> <p>I. T. Kurniawan and W. Adiprawita, \u201dAutonomy Design and Development for an Ultraviolet-C Healthcare Surface Disinfection Robot,\u201d 2021 International Symposium on Electronics and Smart Devices (ISESD), Bandung, Indonesia, 2021, pp. 1-6, doi: 10.1109/ISESD53023.2021.9501737.</p> </li> </ul>"},{"location":"about/#awards","title":"Awards","text":"<ul> <li>Monbukagakusho (MEXT) Scholarship, Japanese Ministry of Education, Culture, Sports, Science and Technology (2024 - 2027)</li> </ul>"},{"location":"blog/","title":"Blog","text":"<p>... of things that happened to be on my mind.</p>"},{"location":"blog/2024/10/30/hello_world/","title":"Hello, World!","text":"<p>Today I decided (on a whim) that I will remake my website. I also decided to grab a domain while I'm at it, considering that I can also use it as a Bluesky handle.</p> <p>My main concerns regarding my old website was that it was hard to maintain and update. I also messed with the template I was using a little too much, making the code even harder to manage.</p> <p>After a quick search on Github, I found this template that allows me to publish my Obsidian notes on Github Pages. This is great for two reasons: I've been logging pretty much everything in Obsidian for the past few years, and I have virtually zero web development skills. After setting everything up, I must say that I am impressed with the template and Material for MkDocs, both are really easy to use yet powerful and customizable.</p> <p>Now that my website is much simpler and maintainable, I kind of want to put my paper reading notes here. Maybe I will also write more blog posts, who knows.</p> <p>Thanks for reading the whole post, see you on the next one!</p>"},{"location":"projects/","title":"Projects","text":"<p>Past research and courseworks that I have done.</p>"},{"location":"projects/#research","title":"Research","text":"<ul> <li> <p> Autonomous Tram</p> <p>An autonomous tram developed in cooperation with the Indonesian Railway Industry Company (PT INKA) and PT Riset Kecerdasan Buatan (PT RKB).</p> </li> <li> <p> ClusterFusion</p> <p>A radar-monocular camera fusion-based 3D object detector for autonomous vehicles. Achieved state-of-the-art performance among similar methods on the nuScenes detection leaderboard.</p> </li> <li> <p> Dagozilla</p> <p>Two generations of autonomous mobile soccer robots designed for the RoboCup Middle Size League.</p> </li> <li> <p> Soca</p> <p>An autonomous ultraviolet-C germicidal irradiation robot. Built in simulated environment using ROS and Gazebo.</p> </li> <li> <p> Soca Octomap</p> <p>A method for simulating ultraviolet germicidal irradiation (UVGI) systems' irradiation coverage in Gazebo, enabling optimal UVGI system static placement or route planning.</p> </li> <li> <p> Dagozilla Telepresence Robot</p> <p>A telepresence robot that is designed to represent its user. The robot can be controlled via the internet, and will stream its user's face and voice.</p> </li> </ul>"},{"location":"projects/#courseworks","title":"Courseworks","text":"<ul> <li> <p> Lidar Odometry</p> <p>A simple scan matching-based 2D lidar odometry Robot Operating System (ROS) node, solved using the iterative closest point (ICP) algorithm and implemented in C++ using Eigen.</p> </li> <li> <p> Self Balancing Robot</p> <p>A two-wheeled self-balancing robot that can keep itself upright and be controlled using a joystick.</p> </li> <li> <p> Security Guard Robot</p> <p>A mini security guard robot that patrols the environment for unknown or wanted faces and warns human guards when one is found.</p> </li> </ul>"},{"location":"projects/autonomous_tram/","title":"Autonomous Tram","text":"<p>An autonomous tram developed in cooperation with the Indonesian Railway Industry Company (PT INKA) and PT Riset Kecerdasan Buatan (PT RKB).</p> <p></p> <p>The tram with cameras, lidar, and radars attached to its front face</p> <p>The autonomous tram was a joint project between Bandung Institute of Technology (ITB), the Indonesian Railway Industry Company (PT INKA), and PT Riset Kecerdasan Buatan (PT RKB) and was funded by the Indonesian Ministry of Finance under the RISPRO LPDP research funding program. I worked on the autonomous tram as a research assistant from ITB during my time as a master's student there. The project aimed to develop a battery-powered autonomous tram for use in the mixed-traffic environment of Surakarta, Indonesia. The tram has to be able to navigate and drive itself safely by relying solely on its onboard sensors and computer, without the help of any marker or tags placed in the environment.</p> <p>The tram carries multiple sensors of different modalities: 9 cameras, a lidar, 2 radars, a GNSS, and an IMU. All the sensors communicate with the tram's NVIDIA DRIVE AGX embedded computer which interfaces with the tram's PLC. In the project, I was responsible for the tram's radar-based perception system, a part of which was my master's thesis, and the tram's overall system integration.</p>"},{"location":"projects/autonomous_tram/#the-radar-based-perception-system","title":"The Radar-Based Perception System","text":"<p>I was responsible for everything related to radar in this project, from the electrical wiring to the deep learning model implementation on embedded systems. In the tram's perception system, the radar is used to help perform 3D object detection due to its ability to measure objects' depth and velocity directly. We used Continental ARS430RDI automotive radars in this project. I wrote my own ethernet packet parser for the radar as Continental did not provide any software to access the data sensed by the radar.</p> <p>Due to the sparse nature of radar point clouds, the accuracy of the current state-of-the-art radar 3D object detection methods is very poor compared to methods that utilize other sensors like cameras or lidars. Thus, radar is often used to support or complement other sensors instead of being used as a standalone sensor. In this project, the radar is paired with a Sekonix SF3325 RGB camera. As radar-camera automatic extrinsic calibration methods are not too well studied and often require a lidar and a custom-made marker, we opted to use a manual calibration method instead. I adapted OpenCalib to our use case, converting the point clouds into clouds of fixed-size pillars to make the calibration process easier and fixed several transformation equations. The radar-camera extrinsic calibration process using our custom software is illustrated below.</p> <p></p> <p>The calibration process using our custom software</p> <p>We implemented CenterFusion, a feature-level fusion approach to radar-camera 3D object detection, on the NVIDIA DRIVE AGX. As some of the key parts of the architecture are not neural networks, they have to be implemented manually so that they can be processed efficiently on the GPU. We adapted the TensorRT implementation by HaohaoNJU to be compatible with our sensor hardware and to use the NVIDIA DriveWorks software framework. We changed the backbone to DLAv0 and added the capability of outputting 2D bounding boxes and velocity. We also incorporated amodal offsets in the postprocessing step for a more accurate localization, especially for objects that are truncated in the image.</p> <p>The embedded implementation was written in C++ and CUDA C++ using the TensorRT framework. It loads a model that was trained on the nuScenes dataset. Due to us not having any annotated radar data at the ready, we did not perform transfer learning on the model. The model training itself was done on an NVIDIA DGX-1 using a PyTorch and OpenCV-based Python model implementation and training script.</p> <p>The embedded implementation can run at 24 FPS on the NVIDIA DRIVE AGX. A sample scene showing the object detection performance of the model on the nuScenes dataset and the new data distribution taken in Bandung, Indonesia is shown below.</p> <p></p> <p>A sample scene showing the object detection performance on the nuScenes dataset. Inferred on the NVIDIA DRIVE AGX at 24 FPS</p> <p></p> <p>A sample scene showing the object detection performance on a new data distribution taken in Bandung, Indonesia. Note that no transfer learning was performed. Inferred on the NVIDIA DRIVE AGX at 24 FPS</p>"},{"location":"projects/autonomous_tram/#the-trams-system-integration","title":"The Tram's System Integration","text":"<p>Aside from the tram's radar-based perception system, I was also responsible for helping integrate the tram's many independent subsystems into a single complex system. Among the subsystems I helped integrate into the main routine were the decision-making and the visualization subsystem. I also wrote the first version of the tram's graphical user interface and image-based decision-making system illustrated below.</p> <p></p> <p>The first version of the tram's graphical user interface</p> <p>Back to Projects</p>"},{"location":"projects/balancing/","title":"Self-Balancing Robot","text":"<p>A two-wheeled self-balancing robot that can keep itself upright and be controlled using a joystick.</p> <p></p> <p>The two-wheeled self-balancing robot</p> <p>The Self-Balancing Robot was built as my EL5214 Embedded Control Systems course final project, taken during my master's at Bandung Institute of Technology (2021). This robot is designed to balance itself upright robustly even in the presence of disturbances. The robot receives motion commands through a wired joystick.</p> <p></p> <p>The robot's main electric circuitry, consisting of an Arduino Nano, an MPU9250 IMU, and an L298N motor driver</p> <p>The robot uses an Arduino Nano as its main computing unit and an MPU9250 IMU as its orientation sensor. The robot is actuated by a pair of JGA25 brushed DC motors controlled by an L298N motor driver. The gyroscope and accelerometer sensor readings from the IMU are fed into a complementary filter to estimate the robot's orientation. </p> <p>The Arduino is running a PID controller to regulate the robot's orientation. A finite state machine (FSM) is used to process joystick inputs into orientation setpoints to be tracked by the PID controller. The PID controller uses the output of the complementary filter as orientation feedback. The controller outputs PWM duty cycles to be given to the motor driver. The dead zone of the motors is compensated by setting the lower bounds of the PWM duty cycles to a tunable value higher than zero.</p> <p>Below are some GIFs showing the robot balancing itself. Even in the presence of disturbances, the robot is able to keep itself upright.</p> <p> </p> <p>The robot balances itself even in the presence of disturbances</p> <p>The robot can also follow motion commands given through the wired joystick as shown in the following GIFs.</p> <p> </p> <p>The robot balances itself while following motion commands given through the joystick</p> <p>Back to Projects</p>"},{"location":"projects/clusterfusion/","title":"ClusterFusion","text":"<p>A radar-monocular camera fusion-based 3D object detector for autonomous vehicles. Achieved state-of-the-art performance among similar methods on the nuScenes detection leaderboard.</p> <p>ClusterFusion is a radar-monocular camera feature-level fusion-based 3D object detector that leverages the local spatial and point-wise features of radar point clouds. I developed ClusterFusion as a part of my master's thesis at Bandung Institute of Technology under the supervision of Prof. Bambang Riyanto Trilaksono.</p> <p></p> <p>A monocular camera image and a projected radar point cloud (left); the same point cloud viewed from above and ground truth object bounding boxes (right). It is easier to approximate the objects' shape from the point cloud in the top view than in the image perspective view</p> <p>As illustrated in the Figure above, the shape of the objects can be approximated by observing the geometry of the radar point cloud. It is relatively harder to do the same in the image perspective view, where the point cloud's depth dimension is flattened. Existing radar-monocular camera methods only extract the point-wise features from the point clouds and ignore the point clouds' geometry which contains information on the position, orientation, and dimensions of the objects. Some other methods project the point clouds onto images and add the sparse radar depth as an additional channel, making it harder to extract the geometric information from the point clouds.</p> <p>ClusterFusion makes the most out of the radar point clouds' local spatial and point-wise features by clustering the radar point cloud and performing feature extraction directly on the radar clusters, minimizing information loss. ClusterFusion uses a CenterFusion-inspired (Nabati and Qi, 2021) frustum-based association mechanism to form radar clusters along with a handcrafted radar cluster feature extraction method. The radar features are then fused with the image features on the image plane to obtain the fused feature maps that are processed further. The general block diagram of the ClusterFusion architecture is shown in the Figure below.</p> <p></p> <p>ClusterFusion's architecture, best viewed in light mode</p> <p>At the time of publication, ClusterFusion achieved the state-of-the-art performance among all the radar-monocular camera methods on the nuScenes' detection leaderboard with a nuScenes detection score (NDS) of 48.7%. ClusterFusion was written in Python using PyTorch and OpenCV. More details regarding ClusterFusion, along with an investigation of the performance of different feature extraction strategies on radar point cloud clusters, can be found in the paper \"ClusterFusion: Leveraging Radar Spatial Features for Radar-Camera 3D Object Detection in Autonomous Vehicles\" (Kurniawan and Trilaksono, 2023). The paper has been published in the open access IEEE Access journal.</p> <p> </p> <p>Sample ClusterFusion detection results on data from the nuScenes dataset</p> <p>Back to Projects</p>"},{"location":"projects/dagozilla/","title":"Dagozilla","text":"<p>Two generations of autonomous mobile soccer robots designed for the RoboCup Middle Size League.</p> <p></p> <p>Dagozilla's third generation autonomous soccer robots</p> <p>Dagozilla is an autonomous mobile robot developer student team from Bandung Institute of Technology. Dagozilla develops autonomous mobile soccer robots to compete in the RoboCup Soccer Middle Size League (MSL). In 2019, I was elected as the team leader of Dagozilla, responsible for leading the development of the team's third-generation autonomous mobile soccer robots. By the end of 2020, we managed to build two striker robots and were qualified for RoboCup MSL 2020. Unfortunately, we did not get to compete as the competition was canceled due to the COVID-19 pandemic.</p> <p>The robots can autonomously play soccer using only the sensors they have onboard. They rely on an omnidirectional camera as their main sensor, supported by other sensors such as rotary encoders and a compass. The robots localize themselves against a map of the field's lines using the Augmented Monte Carlo Localization (AMCL) algorithm.</p> <p>The robots use the A* algorithm to generate path plans. From the path plans, they generate velocity setpoints using a trapezoidal velocity profile, which are then tracked using PID controllers. The robots can kick a soccer ball through the use of a solenoid-based kicking mechanism. The kicking mechanism can deliver both a lob shot and a flat shot. They can also actively control a ball thanks to their dribbling mechanism.</p> <p></p> <p>The robots in action</p> <p>During a match, each robot follows a strategy implemented using behavior trees. Each robot follows a different strategy designed according to its role. This way, the robots can cooperate while being aware of their individual role. The striker robot focuses on offense while the defender robot covers the back area, but they can also both work together in defense or offense when needed.</p> <p>Hardware-wise, the robot uses both an x86-64 PC and an STM32F767ZI microcontroller. The microcontroller runs lower-level control loops and interfaces with the sensors and actuators. The PC handles everything else: communication, behavior, localization, and higher-level control.</p> <p>As the team lead, I was in charge of leading the overall research, development, and manufacture of the robot. Aside from my duties as the team lead, I was in charge of the robot's control system. I designed and implemented the robot's motion and actuator control, from the high-level control algorithms to the low-level embedded hardware control.</p> <p></p> <p>The robot delivering a lob shot in slow-motion</p> <p>When I first began working for the team, the team was improving the second-generation robot's performance. As an electrical engineer, I was responsible for designing the PCBs used in the robots and improving the robot's sensor and control system performance. In the competitions, I was responsible for the defender robot.</p> <p></p> <p>Dagozilla's second generation autonomous soccer robots</p> <p>Back to Projects</p>"},{"location":"projects/guard/","title":"Security Guard Robot","text":"<p>A mini security guard robot that patrols the environment for unknown or wanted faces and warns human guards when one is found.</p> <p></p> <p>The security guard robot</p> <p>The Security Guard Robot was my EL4126 Robotics course final project, taken during my bachelor's at Bandung Institute of Technology (2019). This robot is designed to patrol a predetermined route while performing face detection and recognition. The robot will then make noises and alert the human guards when any unknown face or a wanted face is detected.</p> <p>The robot was developed using the Robot Operating System (ROS) framework, ran on a Raspberry Pi Model 3B+ alongside an STM32 microcontroller. The robot moves around on a holonomic three-wheeled platform. The face detection and recognition is done on footages captured by a Logitech webcam. The face detection is done using Haar cascades and the face within each bounding boxes is then recognized using Eigenfaces. All the computer vision programs is implemented with the help of the OpenCV library.</p> <p>Back to Projects</p>"},{"location":"projects/scan_matching/","title":"Lidar Odometry","text":"<p>A simple scan matching-based 2D lidar odometry Robot Operating System (ROS) node, solved using the iterative closest point (ICP) algorithm and implemented in C++ using Eigen.</p> <p></p> <p>A sample trajectory estimated by the lidar odometry compared against the ground truth trajectory</p> <p>The scan matching-based 2D lidar odometry Robot Operating System (ROS) node was written as the final project for the EL5000 Advanced Mathematics course, taken during my master's at Bandung Institute of Technology (2020). It was written in C++ using the Eigen library and implements the iterative closest point (ICP) algorithm. As a mathematics course final project, this project was written mainly to demonstrate how singular value decomposition (SVD) can be used to solve the point cloud alignment problem and is not meant to be an implementation suitable for real-world use. A proof of concept of the algorithm was implemented in Octave and is illustrated below.</p> <p> </p> <p>Illustrations of the Octave proof of concept implementation of the ICP algorithm</p> <p>The correspondences between the two point clouds are made using naive nearest neighbor searches. All correspondence pairs are weighted equally and no outlier rejection is performed. The translation vector between the point clouds is obtained by calculating the position difference between the centroids of both point clouds. The rotation matrix is extracted using SVD. To accelerate computations, all input point clouds are subsampled beforehand.</p> <p>The ICP C++ implementation is then wrapped in a ROS node which subscribes to an input 2D laser scan topic and publishes the resulting calculated odometry. The implementation is tested on a set of sample 2D lidar data taken in Gazebo and is evaluated against the ground truth trajectory using the evo odometry evaluation package. The following figures show the estimated and ground truth trajectories as well as the position and orientation comparison at every time step.</p> <p> </p> <p>An evaluation of the lidar odometry implementation on a sample trajectory</p> <p>Back to Projects</p>"},{"location":"projects/soca/","title":"Soca","text":"<p>An autonomous ultraviolet-C germicidal irradiation robot. Built in simulated environment using ROS and Gazebo.</p> <p></p> <p>Surface irradiation simulation</p> <p>Soca is my undergraduate thesis, a solo project to develop an autonomy software for ultraviolet-C germicidal irradiation surface disinfection robots. Soca is designed to be modular and hardware agnostic. However, we use a simulated custom-built robot based on the TurtleBot3 platform that carries a 2D Lidar and a Magnetic, Angular Rate, and Gravity (MARG) sensor as our testing platform. To use this robot, the user can simply request the robot to disinfect a room by passing the room name to the robot.</p> <p>To reduce the computation load, Soca does not perform SLAM, and instead uses a fixed map for its localization and navigation needs. The map is built beforehand using Google Cartographer. Soca uses the Augmented Monte Carlo Localization (AMCL) algorithm to localize itself with respect to the map. For its navigation needs, Soca uses the Rapidly Exploring Random Tree* (RRT*) algorithm as its path planner and the Spanning Tree Coverage (STC) algorithm as its coverage path planner.</p> <p></p> <p>The RRT* algorithm in action</p> <p>To reduce unnecessary complexities, we wrote our own C++ implementation of the localization and navigation stack, which includes the AMCL, RRT*, and STC algorithms. We wrote the program using the Robot Operating System (ROS) framework. To accompany the robot, we also wrote a graphical user interface for the user to give the robot commands using Qt. </p> <p>Finally, we tested and evaluated Soca in a simulated environment. As ultraviolet surface irradiation is not traditionally supported in Gazebo, we developed our own method to simulate it, described in greater depth in this paper. We open-sourced our implementation of the simulation method, accessible in this GitHub repository. The design process, implementation details, and disinfection performance evaluation are discussed in more detail in the paper of this project.</p> <p></p> <p>Soca autonomously disinfecting hospital rooms in simulation</p> <p>Back to Projects</p>"},{"location":"projects/telepresence/","title":"Dagozilla Telepresence Robot","text":"<p>A telepresence robot that is designed to represent its user. The robot can be controlled via the internet, and will stream its user's face and voice.</p> <p></p> <p>Dagozilla Telepresence Robot</p> <p>In 2018, I worked on the Dagozilla Telepresence Robot as one of Dagozilla's electrical engineers. The telepresence robot is designed to act as a representative of its user, who operates it remotely, wherever the user may be. The robot can stream the user's face and voice while being controlled through the internet. Its tablet height can be adjusted by the user to suit the user's needs. The user can use the robot for various purposes such as attending a meeting and checking around the environment.</p> <p>The robot uses both a Raspberry Pi 3B+ and an STM32F446RE microcontroller as its computing units. Its neck supports a Xiaomi tablet which acts as its user interface.</p> <p></p> <p>The telepresence robot in Bandung Institute of Technology Open House 2019</p> <p>As an electrical engineer, I was in charge of the robot's overall electrical system. Specifically, I was responsible for designing, manufacturing, and implementing its tablet height adjustment electrical systems and control. I also implemented the robot's odometry system, I designed the PCBs and wrote the code for the sensor's data acquisition routine. Through the robot's manufacturing process, I actively participated in the robot's assembly and wiring process.</p> <p>Back to Projects</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/thoughts/","title":"thoughts","text":""}]}